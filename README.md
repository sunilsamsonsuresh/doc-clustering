## Synthetic Insurance Document Clustering Demo (DiT+ HDBSCAN)

## How to Contribute:

### 1) Install
```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
# Mac/Linux: source .venv/bin/activate
pip install -r requirements.txt
```

---

# ğŸ“„ Document Clustering & Classification Lab

This repository is an experimental framework for **vision-based document understanding**, focused on:

* ğŸ“¦ Unsupervised document clustering
* ğŸ§  Layout-driven embeddings (no OCR required)
* ğŸ— Template and document-family discovery
* ğŸ¥ Insurance-style document intake use cases (FNOL, claims, policies, receipts, correspondence)

It supports multiple state-of-the-art document vision models and a modular pipeline to go from **raw documents â†’ embeddings â†’ clusters â†’ organized folders**.

---

# ğŸ¯ What this project does

âœ” Convert mixed documents into standardized first-page images
âœ” Embed documents using modern vision/document models
âœ” Cluster documents using UMAP + HDBSCAN
âœ” Organize documents into cluster folders
âœ” Evaluate clustering quality when labels are available
âœ” Serve as a foundation for building document classifiers

---

# ğŸ§  Supported models

The framework currently supports:

| Model                                | Type                 | OCR | Best for                                   |
| ------------------------------------ | -------------------- | --- | ------------------------------------------ |
| **DiT (Document Image Transformer)** | Document vision      | âŒ   | **Primary model for layout clustering**    |
| DINOv2 (small/base/large)            | Foundation vision    | âŒ   | Strong general layout & template discovery |
| LayoutLMv3                           | Multimodal doc model | âœ…   | Layout + text-aware embeddings             |
| Donut (encoder only)                 | Doc understanding    | âŒ   | Heavy but document-specialized             |

â¡ In practice, **DiT gave the strongest and most stable clustering results** for large unlabeled document sets.

---

# ğŸ— Repository structure

```
doc-clustering/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ <source_folders>/        # invoice/, form/, fnol/, receipts/, etc.
â”‚   â”œâ”€â”€ page1_pngs/               # normalized first-page images
â”‚   â”œâ”€â”€ raw_pdfs/                 # wrapped PDFs for cluster delivery
â”‚   â””â”€â”€ meta.csv                  # document registry
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ embeddings_*.npy
â”‚   â”œâ”€â”€ umap_*.npy
â”‚   â”œâ”€â”€ clusters_*.csv
â”‚   â””â”€â”€ clusters/                 # cluster folders with PDFs
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prepare_local_rvl.py      # dataset preparation
â”‚   â”œâ”€â”€ cluster_dinov2.py
â”‚   â”œâ”€â”€ cluster_layoutlmv3.py
â”‚   â”œâ”€â”€ cluster_donut.py
â”‚   â”œâ”€â”€ cluster_dit.py            # â­ recommended
â”‚   â”œâ”€â”€ organize_clusters.py
â”‚   â””â”€â”€ config.py
â”‚
â””â”€â”€ README.md
```

---

# ğŸ”„ End-to-end pipeline

## 1ï¸âƒ£ Prepare dataset (ingestion layer)

Organize your raw documents like:

```
data/
  fnol/
  claim_form/
  invoice/
  receipt/
  correspondence/
```

Then run:

```bash
python3 -m src.prepare_local_rvl
```

This step:

* samples documents
* converts first pages to PNG
* wraps images into single-page PDFs
* builds `meta.csv`

Outputs:

```
data/page1_pngs/
data/raw_pdfs/
data/meta.csv
```

This layer decouples **ingestion** from **ML models**.

---

## 2ï¸âƒ£ Generate embeddings + clusters

### â­ DiT (recommended)

```bash
python3 -m src.cluster_dit
```

### DINOv2

```bash
python3 -m src.cluster_dinov2
```

### LayoutLMv3 (OCR + layout)

```bash
python3 -m src.cluster_layoutlmv3
```

### Donut encoder

```bash
python3 -m src.cluster_donut
```

Each script:

* embeds first-page images
* reduces dimensionality with UMAP
* clusters using HDBSCAN
* writes cluster labels to `outputs/clusters_*.csv`

---

## 3ï¸âƒ£ Organize documents into cluster folders

```bash
python3 -m src.organize_clusters
```

Creates:

```
outputs/clusters/
  cluster_0/
  cluster_1/
  cluster_2/
```

Each folder contains **PDFs of documents belonging to that cluster** â€” ideal for:

* visual inspection
* stakeholder demos
* manual labeling
* downstream automation

---

# ğŸ¥ Insurance-style use case

This framework is designed for scenarios such as:

* FNOL form discovery
* Claim document grouping
* Policy vs invoice vs correspondence separation
* Template family detection
* Intake triage and automation

Typical flow:

```
Unlabeled documents
   â†“
DiT embeddings
   â†“
HDBSCAN clusters
   â†“
Human review & naming
   â†“
Training set creation
   â†“
Supervised document classifier
```

This turns unsupervised discovery into a **production-grade document classification system**.

---

# ğŸ“Š Evaluation

If `meta.csv` contains `doc_type`, clustering scripts automatically report:

* ARI (Adjusted Rand Index)
* NMI (Normalized Mutual Information)
* number of clusters
* noise ratio

For truly unlabeled corpora, evaluation is visual and cluster-purity driven.

---

# âš™ Configuration

All core settings live in `src/config.py`:

```python
# Models
dinov2_model = "facebook/dinov2-large"
dit_model    = "microsoft/dit-base-finetuned-rvlcdip"

# Embedding
batch_size = 16
device = "auto"

# UMAP
umap_n_components = 15
umap_n_neighbors = 35
umap_min_dist = 0.1

# HDBSCAN
hdb_min_cluster_size = 30
hdb_min_samples = 8
```

---

# ğŸš€ Recommended setup for large unlabeled corpora

For 3kâ€“10k documents:

```
Model: DiT
UMAP neighbors: 30â€“50
Min cluster size: 30â€“60
Min samples: 6â€“12
```

---

# ğŸ§© Why this architecture

* Vision-first (no OCR dependency)
* Template and layout sensitive
* Scales to thousands of documents
* Model-agnostic embedding layer
* Business-friendly cluster outputs
* Natural bridge to supervised classification

---

# ğŸ›£ Roadmap ideas

* cluster thumbnails & HTML browser
* hybrid vision + OCR embeddings
* reclustering large clusters
* active-learning loop
* classifier training pipeline
* Databricks / cloud batch mode

---

# ğŸ“Œ Key takeaway

> This project is not just clustering â€” it is a **document understanding pipeline** for discovering, structuring, and operationalizing large unlabeled document collections.
